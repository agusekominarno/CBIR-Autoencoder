{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Batik Autoencoder Image Retrieval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agusekominarno/CBIR-Batik-Autoencoder/blob/main/Batik_Autoencoder_Image_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBX7HF2o09XV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec5292a6-0f1a-43fa-a502-715ebb4c4caf"
      },
      "source": [
        "!git clone https://github.com/agusekominarno/Batik.git\n",
        "!unzip Batik/Dataset/Batik300.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Batik'...\n",
            "remote: Enumerating objects: 217, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 217 (delta 4), reused 0 (delta 0), pack-reused 205\u001b[K\n",
            "Receiving objects: 100% (217/217), 14.48 MiB | 22.57 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n",
            "Archive:  Batik/Dataset/Batik300.zip\n",
            "  inflating: Batik300/B1_1.jpg       \n",
            "  inflating: Batik300/B1_2.jpg       \n",
            "  inflating: Batik300/B1_3.jpg       \n",
            "  inflating: Batik300/B1_4.jpg       \n",
            "  inflating: Batik300/B1_5.jpg       \n",
            "  inflating: Batik300/B1_6.jpg       \n",
            "  inflating: Batik300/B10_1.jpg      \n",
            "  inflating: Batik300/B10_2.jpg      \n",
            "  inflating: Batik300/B10_3.jpg      \n",
            "  inflating: Batik300/B10_4.jpg      \n",
            "  inflating: Batik300/B10_5.jpg      \n",
            "  inflating: Batik300/B10_6.jpg      \n",
            "  inflating: Batik300/B11_1.jpg      \n",
            "  inflating: Batik300/B11_2.jpg      \n",
            "  inflating: Batik300/B11_3.jpg      \n",
            "  inflating: Batik300/B11_4.jpg      \n",
            "  inflating: Batik300/B11_5.jpg      \n",
            "  inflating: Batik300/B11_6.jpg      \n",
            "  inflating: Batik300/B12_1.jpg      \n",
            "  inflating: Batik300/B12_2.jpg      \n",
            "  inflating: Batik300/B12_3.jpg      \n",
            "  inflating: Batik300/B12_4.jpg      \n",
            "  inflating: Batik300/B12_5.jpg      \n",
            "  inflating: Batik300/B12_6.jpg      \n",
            "  inflating: Batik300/B13_1.jpg      \n",
            "  inflating: Batik300/B13_2.jpg      \n",
            "  inflating: Batik300/B13_3.jpg      \n",
            "  inflating: Batik300/B13_4.jpg      \n",
            "  inflating: Batik300/B13_5.jpg      \n",
            "  inflating: Batik300/B13_6.jpg      \n",
            "  inflating: Batik300/B14_1.jpg      \n",
            "  inflating: Batik300/B14_2.jpg      \n",
            "  inflating: Batik300/B14_3.jpg      \n",
            "  inflating: Batik300/B14_4.jpg      \n",
            "  inflating: Batik300/B14_5.jpg      \n",
            "  inflating: Batik300/B14_6.jpg      \n",
            "  inflating: Batik300/B15_1.jpg      \n",
            "  inflating: Batik300/B15_2.jpg      \n",
            "  inflating: Batik300/B15_3.jpg      \n",
            "  inflating: Batik300/B15_4.jpg      \n",
            "  inflating: Batik300/B15_5.jpg      \n",
            "  inflating: Batik300/B15_6.jpg      \n",
            "  inflating: Batik300/B16_1.jpg      \n",
            "  inflating: Batik300/B16_2.jpg      \n",
            "  inflating: Batik300/B16_3.jpg      \n",
            "  inflating: Batik300/B16_4.jpg      \n",
            "  inflating: Batik300/B16_5.jpg      \n",
            "  inflating: Batik300/B16_6.jpg      \n",
            "  inflating: Batik300/B17_1.jpg      \n",
            "  inflating: Batik300/B17_2.jpg      \n",
            "  inflating: Batik300/B17_3.jpg      \n",
            "  inflating: Batik300/B17_4.jpg      \n",
            "  inflating: Batik300/B17_5.jpg      \n",
            "  inflating: Batik300/B17_6.jpg      \n",
            "  inflating: Batik300/B18_1.jpg      \n",
            "  inflating: Batik300/B18_2.jpg      \n",
            "  inflating: Batik300/B18_3.jpg      \n",
            "  inflating: Batik300/B18_4.jpg      \n",
            "  inflating: Batik300/B18_5.jpg      \n",
            "  inflating: Batik300/B18_6.jpg      \n",
            "  inflating: Batik300/B19_1.jpg      \n",
            "  inflating: Batik300/B19_2.jpg      \n",
            "  inflating: Batik300/B19_3.jpg      \n",
            "  inflating: Batik300/B19_4.jpg      \n",
            "  inflating: Batik300/B19_5.jpg      \n",
            "  inflating: Batik300/B19_6.jpg      \n",
            "  inflating: Batik300/B2_1.jpg       \n",
            "  inflating: Batik300/B2_2.jpg       \n",
            "  inflating: Batik300/B2_3.jpg       \n",
            "  inflating: Batik300/B2_4.jpg       \n",
            "  inflating: Batik300/B2_5.jpg       \n",
            "  inflating: Batik300/B2_6.jpg       \n",
            "  inflating: Batik300/B20_1.jpg      \n",
            "  inflating: Batik300/B20_2.jpg      \n",
            "  inflating: Batik300/B20_3.jpg      \n",
            "  inflating: Batik300/B20_4.jpg      \n",
            "  inflating: Batik300/B20_5.jpg      \n",
            "  inflating: Batik300/B20_6.jpg      \n",
            "  inflating: Batik300/B21_1.jpg      \n",
            "  inflating: Batik300/B21_2.jpg      \n",
            "  inflating: Batik300/B21_3.jpg      \n",
            "  inflating: Batik300/B21_4.jpg      \n",
            "  inflating: Batik300/B21_5.jpg      \n",
            "  inflating: Batik300/B21_6.jpg      \n",
            "  inflating: Batik300/B22_1.jpg      \n",
            "  inflating: Batik300/B22_2.jpg      \n",
            "  inflating: Batik300/B22_3.jpg      \n",
            "  inflating: Batik300/B22_4.jpg      \n",
            "  inflating: Batik300/B22_5.jpg      \n",
            "  inflating: Batik300/B22_6.jpg      \n",
            "  inflating: Batik300/B23_1.jpg      \n",
            "  inflating: Batik300/B23_2.jpg      \n",
            "  inflating: Batik300/B23_3.jpg      \n",
            "  inflating: Batik300/B23_4.jpg      \n",
            "  inflating: Batik300/B23_5.jpg      \n",
            "  inflating: Batik300/B23_6.jpg      \n",
            "  inflating: Batik300/B24_1.jpg      \n",
            "  inflating: Batik300/B24_2.jpg      \n",
            "  inflating: Batik300/B24_3.jpg      \n",
            "  inflating: Batik300/B24_4.jpg      \n",
            "  inflating: Batik300/B24_5.jpg      \n",
            "  inflating: Batik300/B24_6.jpg      \n",
            "  inflating: Batik300/B25_1.jpg      \n",
            "  inflating: Batik300/B25_2.jpg      \n",
            "  inflating: Batik300/B25_3.jpg      \n",
            "  inflating: Batik300/B25_4.jpg      \n",
            "  inflating: Batik300/B25_5.jpg      \n",
            "  inflating: Batik300/B25_6.jpg      \n",
            "  inflating: Batik300/B26_1.jpg      \n",
            "  inflating: Batik300/B26_2.jpg      \n",
            "  inflating: Batik300/B26_3.jpg      \n",
            "  inflating: Batik300/B26_4.jpg      \n",
            "  inflating: Batik300/B26_5.jpg      \n",
            "  inflating: Batik300/B26_6.jpg      \n",
            "  inflating: Batik300/B27_1.jpg      \n",
            "  inflating: Batik300/B27_2.jpg      \n",
            "  inflating: Batik300/B27_3.jpg      \n",
            "  inflating: Batik300/B27_4.jpg      \n",
            "  inflating: Batik300/B27_5.jpg      \n",
            "  inflating: Batik300/B27_6.jpg      \n",
            "  inflating: Batik300/B28_1.jpg      \n",
            "  inflating: Batik300/B28_2.jpg      \n",
            "  inflating: Batik300/B28_3.jpg      \n",
            "  inflating: Batik300/B28_4.jpg      \n",
            "  inflating: Batik300/B28_5.jpg      \n",
            "  inflating: Batik300/B28_6.jpg      \n",
            "  inflating: Batik300/B29_1.jpg      \n",
            "  inflating: Batik300/B29_2.jpg      \n",
            "  inflating: Batik300/B29_3.jpg      \n",
            "  inflating: Batik300/B29_4.jpg      \n",
            "  inflating: Batik300/B29_5.jpg      \n",
            "  inflating: Batik300/B29_6.jpg      \n",
            "  inflating: Batik300/B3_1.jpg       \n",
            "  inflating: Batik300/B3_2.jpg       \n",
            "  inflating: Batik300/B3_3.jpg       \n",
            "  inflating: Batik300/B3_4.jpg       \n",
            "  inflating: Batik300/B3_5.jpg       \n",
            "  inflating: Batik300/B3_6.jpg       \n",
            "  inflating: Batik300/B30_1.jpg      \n",
            "  inflating: Batik300/B30_2.jpg      \n",
            "  inflating: Batik300/B30_3.jpg      \n",
            "  inflating: Batik300/B30_4.jpg      \n",
            "  inflating: Batik300/B30_5.jpg      \n",
            "  inflating: Batik300/B30_6.jpg      \n",
            "  inflating: Batik300/B31_1.jpg      \n",
            "  inflating: Batik300/B31_2.jpg      \n",
            "  inflating: Batik300/B31_3.jpg      \n",
            "  inflating: Batik300/B31_4.jpg      \n",
            "  inflating: Batik300/B31_5.jpg      \n",
            "  inflating: Batik300/B31_6.jpg      \n",
            "  inflating: Batik300/B32_1.jpg      \n",
            "  inflating: Batik300/B32_2.jpg      \n",
            "  inflating: Batik300/B32_3.jpg      \n",
            "  inflating: Batik300/B32_4.jpg      \n",
            "  inflating: Batik300/B32_5.jpg      \n",
            "  inflating: Batik300/B32_6.jpg      \n",
            "  inflating: Batik300/B33_1.jpg      \n",
            "  inflating: Batik300/B33_2.jpg      \n",
            "  inflating: Batik300/B33_3.jpg      \n",
            "  inflating: Batik300/B33_4.jpg      \n",
            "  inflating: Batik300/B33_5.jpg      \n",
            "  inflating: Batik300/B33_6.jpg      \n",
            "  inflating: Batik300/B34_1.jpg      \n",
            "  inflating: Batik300/B34_2.jpg      \n",
            "  inflating: Batik300/B34_3.jpg      \n",
            "  inflating: Batik300/B34_4.jpg      \n",
            "  inflating: Batik300/B34_5.jpg      \n",
            "  inflating: Batik300/B34_6.jpg      \n",
            "  inflating: Batik300/B35_1.jpg      \n",
            "  inflating: Batik300/B35_2.jpg      \n",
            "  inflating: Batik300/B35_3.jpg      \n",
            "  inflating: Batik300/B35_4.jpg      \n",
            "  inflating: Batik300/B35_5.jpg      \n",
            "  inflating: Batik300/B35_6.jpg      \n",
            "  inflating: Batik300/B36_1.jpg      \n",
            "  inflating: Batik300/B36_2.jpg      \n",
            "  inflating: Batik300/B36_3.jpg      \n",
            "  inflating: Batik300/B36_4.jpg      \n",
            "  inflating: Batik300/B36_5.jpg      \n",
            "  inflating: Batik300/B36_6.jpg      \n",
            "  inflating: Batik300/B37_1.jpg      \n",
            "  inflating: Batik300/B37_2.jpg      \n",
            "  inflating: Batik300/B37_3.jpg      \n",
            "  inflating: Batik300/B37_4.jpg      \n",
            "  inflating: Batik300/B37_5.jpg      \n",
            "  inflating: Batik300/B37_6.jpg      \n",
            "  inflating: Batik300/B38_1.jpg      \n",
            "  inflating: Batik300/B38_2.jpg      \n",
            "  inflating: Batik300/B38_3.jpg      \n",
            "  inflating: Batik300/B38_4.jpg      \n",
            "  inflating: Batik300/B38_5.jpg      \n",
            "  inflating: Batik300/B38_6.jpg      \n",
            "  inflating: Batik300/B39_1.jpg      \n",
            "  inflating: Batik300/B39_2.jpg      \n",
            "  inflating: Batik300/B39_3.jpg      \n",
            "  inflating: Batik300/B39_4.jpg      \n",
            "  inflating: Batik300/B39_5.jpg      \n",
            "  inflating: Batik300/B39_6.jpg      \n",
            "  inflating: Batik300/B4_1.jpg       \n",
            "  inflating: Batik300/B4_2.jpg       \n",
            "  inflating: Batik300/B4_3.jpg       \n",
            "  inflating: Batik300/B4_4.jpg       \n",
            "  inflating: Batik300/B4_5.jpg       \n",
            "  inflating: Batik300/B4_6.jpg       \n",
            "  inflating: Batik300/B40_1.jpg      \n",
            "  inflating: Batik300/B40_2.jpg      \n",
            "  inflating: Batik300/B40_3.jpg      \n",
            "  inflating: Batik300/B40_4.jpg      \n",
            "  inflating: Batik300/B40_5.jpg      \n",
            "  inflating: Batik300/B40_6.jpg      \n",
            "  inflating: Batik300/B41_1.jpg      \n",
            "  inflating: Batik300/B41_2.jpg      \n",
            "  inflating: Batik300/B41_3.jpg      \n",
            "  inflating: Batik300/B41_4.jpg      \n",
            "  inflating: Batik300/B41_5.jpg      \n",
            "  inflating: Batik300/B41_6.jpg      \n",
            "  inflating: Batik300/B42_1.jpg      \n",
            "  inflating: Batik300/B42_2.jpg      \n",
            "  inflating: Batik300/B42_3.jpg      \n",
            "  inflating: Batik300/B42_4.jpg      \n",
            "  inflating: Batik300/B42_5.jpg      \n",
            "  inflating: Batik300/B42_6.jpg      \n",
            "  inflating: Batik300/B43_1.jpg      \n",
            "  inflating: Batik300/B43_2.jpg      \n",
            "  inflating: Batik300/B43_3.jpg      \n",
            "  inflating: Batik300/B43_4.jpg      \n",
            "  inflating: Batik300/B43_5.jpg      \n",
            "  inflating: Batik300/B43_6.jpg      \n",
            "  inflating: Batik300/B44_1.jpg      \n",
            "  inflating: Batik300/B44_2.jpg      \n",
            "  inflating: Batik300/B44_3.jpg      \n",
            "  inflating: Batik300/B44_4.jpg      \n",
            "  inflating: Batik300/B44_5.jpg      \n",
            "  inflating: Batik300/B44_6.jpg      \n",
            "  inflating: Batik300/B45_1.jpg      \n",
            "  inflating: Batik300/B45_2.jpg      \n",
            "  inflating: Batik300/B45_3.jpg      \n",
            "  inflating: Batik300/B45_4.jpg      \n",
            "  inflating: Batik300/B45_5.jpg      \n",
            "  inflating: Batik300/B45_6.jpg      \n",
            "  inflating: Batik300/B46_1.jpg      \n",
            "  inflating: Batik300/B46_2.jpg      \n",
            "  inflating: Batik300/B46_3.jpg      \n",
            "  inflating: Batik300/B46_4.jpg      \n",
            "  inflating: Batik300/B46_5.jpg      \n",
            "  inflating: Batik300/B46_6.jpg      \n",
            "  inflating: Batik300/B47_1.jpg      \n",
            "  inflating: Batik300/B47_2.jpg      \n",
            "  inflating: Batik300/B47_3.jpg      \n",
            "  inflating: Batik300/B47_4.jpg      \n",
            "  inflating: Batik300/B47_5.jpg      \n",
            "  inflating: Batik300/B47_6.jpg      \n",
            "  inflating: Batik300/B48_1.jpg      \n",
            "  inflating: Batik300/B48_2.jpg      \n",
            "  inflating: Batik300/B48_3.jpg      \n",
            "  inflating: Batik300/B48_4.jpg      \n",
            "  inflating: Batik300/B48_5.jpg      \n",
            "  inflating: Batik300/B48_6.jpg      \n",
            "  inflating: Batik300/B49_1.jpg      \n",
            "  inflating: Batik300/B49_2.jpg      \n",
            "  inflating: Batik300/B49_3.jpg      \n",
            "  inflating: Batik300/B49_4.jpg      \n",
            "  inflating: Batik300/B49_5.jpg      \n",
            "  inflating: Batik300/B49_6.jpg      \n",
            "  inflating: Batik300/B5_1.jpg       \n",
            "  inflating: Batik300/B5_2.jpg       \n",
            "  inflating: Batik300/B5_3.jpg       \n",
            "  inflating: Batik300/B5_4.jpg       \n",
            "  inflating: Batik300/B5_5.jpg       \n",
            "  inflating: Batik300/B5_6.jpg       \n",
            "  inflating: Batik300/B50_1.jpg      \n",
            "  inflating: Batik300/B50_2.jpg      \n",
            "  inflating: Batik300/B50_3.jpg      \n",
            "  inflating: Batik300/B50_4.jpg      \n",
            "  inflating: Batik300/B50_5.jpg      \n",
            "  inflating: Batik300/B50_6.jpg      \n",
            "  inflating: Batik300/B6_1.jpg       \n",
            "  inflating: Batik300/B6_2.jpg       \n",
            "  inflating: Batik300/B6_3.jpg       \n",
            "  inflating: Batik300/B6_4.jpg       \n",
            "  inflating: Batik300/B6_5.jpg       \n",
            "  inflating: Batik300/B6_6.jpg       \n",
            "  inflating: Batik300/B7_1.jpg       \n",
            "  inflating: Batik300/B7_2.jpg       \n",
            "  inflating: Batik300/B7_3.jpg       \n",
            "  inflating: Batik300/B7_4.jpg       \n",
            "  inflating: Batik300/B7_5.jpg       \n",
            "  inflating: Batik300/B7_6.jpg       \n",
            "  inflating: Batik300/B8_1.jpg       \n",
            "  inflating: Batik300/B8_2.jpg       \n",
            "  inflating: Batik300/B8_3.jpg       \n",
            "  inflating: Batik300/B8_4.jpg       \n",
            "  inflating: Batik300/B8_5.jpg       \n",
            "  inflating: Batik300/B8_6.jpg       \n",
            "  inflating: Batik300/B9_1.jpg       \n",
            "  inflating: Batik300/B9_2.jpg       \n",
            "  inflating: Batik300/B9_3.jpg       \n",
            "  inflating: Batik300/B9_4.jpg       \n",
            "  inflating: Batik300/B9_5.jpg       \n",
            "  inflating: Batik300/B9_6.jpg       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQyoxAccGH-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef2b24b-5c3b-4d5e-aee8-29d72da97f61"
      },
      "source": [
        "!mkdir data\n",
        "!mkdir data/test \n",
        "!mv Batik300/*_1.jpg data/test\n",
        "!ls data/test"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B10_1.jpg  B17_1.jpg  B24_1.jpg  B3_1.jpg   B39_1.jpg  B46_1.jpg  B8_1.jpg\n",
            "B11_1.jpg  B18_1.jpg  B25_1.jpg  B32_1.jpg  B40_1.jpg  B47_1.jpg  B9_1.jpg\n",
            "B1_1.jpg   B19_1.jpg  B26_1.jpg  B33_1.jpg  B41_1.jpg  B48_1.jpg\n",
            "B12_1.jpg  B20_1.jpg  B27_1.jpg  B34_1.jpg  B4_1.jpg   B49_1.jpg\n",
            "B13_1.jpg  B21_1.jpg  B28_1.jpg  B35_1.jpg  B42_1.jpg  B50_1.jpg\n",
            "B14_1.jpg  B2_1.jpg   B29_1.jpg  B36_1.jpg  B43_1.jpg  B5_1.jpg\n",
            "B15_1.jpg  B22_1.jpg  B30_1.jpg  B37_1.jpg  B44_1.jpg  B6_1.jpg\n",
            "B16_1.jpg  B23_1.jpg  B31_1.jpg  B38_1.jpg  B45_1.jpg  B7_1.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCqxDp7nGH-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783aa229-f523-42b1-818d-f37efa3d26a1"
      },
      "source": [
        "!mkdir data/train \n",
        "!mv Batik300/*.jpg data/train\n",
        "!ls data/train"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B10_2.jpg  B16_4.jpg  B23_3.jpg  B29_5.jpg  B36_2.jpg  B4_2.jpg   B49_3.jpg\n",
            "B10_3.jpg  B16_5.jpg  B23_4.jpg  B29_6.jpg  B36_3.jpg  B43_2.jpg  B49_4.jpg\n",
            "B10_4.jpg  B16_6.jpg  B23_5.jpg  B30_2.jpg  B36_4.jpg  B43_3.jpg  B49_5.jpg\n",
            "B10_5.jpg  B1_6.jpg   B23_6.jpg  B30_3.jpg  B36_5.jpg  B43_4.jpg  B49_6.jpg\n",
            "B10_6.jpg  B17_2.jpg  B2_3.jpg\t B30_4.jpg  B36_6.jpg  B43_5.jpg  B50_2.jpg\n",
            "B11_2.jpg  B17_3.jpg  B24_2.jpg  B30_5.jpg  B3_6.jpg   B43_6.jpg  B50_3.jpg\n",
            "B11_3.jpg  B17_4.jpg  B24_3.jpg  B30_6.jpg  B37_2.jpg  B4_3.jpg   B50_4.jpg\n",
            "B11_4.jpg  B17_5.jpg  B24_4.jpg  B31_2.jpg  B37_3.jpg  B44_2.jpg  B50_5.jpg\n",
            "B11_5.jpg  B17_6.jpg  B24_5.jpg  B31_3.jpg  B37_4.jpg  B44_3.jpg  B50_6.jpg\n",
            "B11_6.jpg  B18_2.jpg  B24_6.jpg  B31_4.jpg  B37_5.jpg  B44_4.jpg  B5_2.jpg\n",
            "B12_2.jpg  B18_3.jpg  B2_4.jpg\t B31_5.jpg  B37_6.jpg  B44_5.jpg  B5_3.jpg\n",
            "B12_3.jpg  B18_4.jpg  B25_2.jpg  B31_6.jpg  B38_2.jpg  B44_6.jpg  B5_4.jpg\n",
            "B12_4.jpg  B18_5.jpg  B25_3.jpg  B32_2.jpg  B38_3.jpg  B4_4.jpg   B5_5.jpg\n",
            "B12_5.jpg  B18_6.jpg  B25_4.jpg  B32_3.jpg  B38_4.jpg  B45_2.jpg  B5_6.jpg\n",
            "B12_6.jpg  B19_2.jpg  B25_5.jpg  B32_4.jpg  B38_5.jpg  B45_3.jpg  B6_2.jpg\n",
            "B1_2.jpg   B19_3.jpg  B25_6.jpg  B32_5.jpg  B38_6.jpg  B45_4.jpg  B6_3.jpg\n",
            "B13_2.jpg  B19_4.jpg  B2_5.jpg\t B32_6.jpg  B39_2.jpg  B45_5.jpg  B6_4.jpg\n",
            "B13_3.jpg  B19_5.jpg  B26_2.jpg  B3_2.jpg   B39_3.jpg  B45_6.jpg  B6_5.jpg\n",
            "B13_4.jpg  B19_6.jpg  B26_3.jpg  B33_2.jpg  B39_4.jpg  B4_5.jpg   B6_6.jpg\n",
            "B13_5.jpg  B20_2.jpg  B26_4.jpg  B33_3.jpg  B39_5.jpg  B46_2.jpg  B7_2.jpg\n",
            "B13_6.jpg  B20_3.jpg  B26_5.jpg  B33_4.jpg  B39_6.jpg  B46_3.jpg  B7_3.jpg\n",
            "B1_3.jpg   B20_4.jpg  B26_6.jpg  B33_5.jpg  B40_2.jpg  B46_4.jpg  B7_4.jpg\n",
            "B14_2.jpg  B20_5.jpg  B2_6.jpg\t B33_6.jpg  B40_3.jpg  B46_5.jpg  B7_5.jpg\n",
            "B14_3.jpg  B20_6.jpg  B27_2.jpg  B3_3.jpg   B40_4.jpg  B46_6.jpg  B7_6.jpg\n",
            "B14_4.jpg  B21_2.jpg  B27_3.jpg  B34_2.jpg  B40_5.jpg  B4_6.jpg   B8_2.jpg\n",
            "B14_5.jpg  B21_3.jpg  B27_4.jpg  B34_3.jpg  B40_6.jpg  B47_2.jpg  B8_3.jpg\n",
            "B14_6.jpg  B21_4.jpg  B27_5.jpg  B34_4.jpg  B41_2.jpg  B47_3.jpg  B8_4.jpg\n",
            "B1_4.jpg   B21_5.jpg  B27_6.jpg  B34_5.jpg  B41_3.jpg  B47_4.jpg  B8_5.jpg\n",
            "B15_2.jpg  B21_6.jpg  B28_2.jpg  B34_6.jpg  B41_4.jpg  B47_5.jpg  B8_6.jpg\n",
            "B15_3.jpg  B22_2.jpg  B28_3.jpg  B3_4.jpg   B41_5.jpg  B47_6.jpg  B9_2.jpg\n",
            "B15_4.jpg  B22_3.jpg  B28_4.jpg  B35_2.jpg  B41_6.jpg  B48_2.jpg  B9_3.jpg\n",
            "B15_5.jpg  B22_4.jpg  B28_5.jpg  B35_3.jpg  B42_2.jpg  B48_3.jpg  B9_4.jpg\n",
            "B15_6.jpg  B22_5.jpg  B28_6.jpg  B35_4.jpg  B42_3.jpg  B48_4.jpg  B9_5.jpg\n",
            "B1_5.jpg   B22_6.jpg  B29_2.jpg  B35_5.jpg  B42_4.jpg  B48_5.jpg  B9_6.jpg\n",
            "B16_2.jpg  B2_2.jpg   B29_3.jpg  B35_6.jpg  B42_5.jpg  B48_6.jpg\n",
            "B16_3.jpg  B23_2.jpg  B29_4.jpg  B3_5.jpg   B42_6.jpg  B49_2.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSyZjZxfD5Z3"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import offsetbox\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "from sklearn import manifold\n",
        "\n",
        "class CV_plot_utils:\n",
        "  # Plot image\n",
        "  def plot_img(img, range=[0, 255]):\n",
        "      plt.imshow(img, vmin=range[0], vmax=range[1])\n",
        "      plt.xlabel(\"xpixels\")\n",
        "      plt.ylabel(\"ypixels\")\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "      plt.close()\n",
        "\n",
        "  # Plots images in 2 rows: top row is query, bottom row is answer\n",
        "  def plot_query_retrieval(img_query, imgs_retrieval, outFile):\n",
        "      n_retrieval = len(imgs_retrieval)\n",
        "      fig = plt.figure(figsize=(2*n_retrieval, 4))\n",
        "      fig.suptitle(\"Image Retrieval (k={})\".format(n_retrieval), fontsize=25)\n",
        "\n",
        "      # Plot query image\n",
        "      ax = plt.subplot(2, n_retrieval, 0 + 1)\n",
        "      plt.imshow(img_query)\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)\n",
        "      for axis in ['top', 'bottom', 'left', 'right']:\n",
        "          ax.spines[axis].set_linewidth(4)  # increase border thickness\n",
        "          ax.spines[axis].set_color('black')  # set to black\n",
        "      ax.set_title(\"query\",  fontsize=14)  # set subplot title\n",
        "\n",
        "      # Plot retrieval images\n",
        "      for i, img in enumerate(imgs_retrieval):\n",
        "          ax = plt.subplot(2, n_retrieval, n_retrieval + i + 1)\n",
        "          plt.imshow(img)\n",
        "          ax.get_xaxis().set_visible(False)\n",
        "          ax.get_yaxis().set_visible(False)\n",
        "          for axis in ['top', 'bottom', 'left', 'right']:\n",
        "              ax.spines[axis].set_linewidth(1)  # set border thickness\n",
        "              ax.spines[axis].set_color('black')  # set to black\n",
        "          ax.set_title(\"Rank #%d\" % (i+1), fontsize=14)  # set subplot title\n",
        "\n",
        "      if outFile is None:\n",
        "          plt.show()\n",
        "      else:\n",
        "          plt.savefig(outFile, bbox_inches='tight')\n",
        "      plt.close()\n",
        "\n",
        "  # Plot t-SNE of images\n",
        "  def plot_tsne(X, imgs, outFile):\n",
        "\n",
        "      def imscatter(x, y, images, ax=None, zoom=1.0):\n",
        "          if ax is None:\n",
        "              ax = plt.gca()\n",
        "          x, y = np.atleast_1d(x, y)\n",
        "          artists = []\n",
        "          for x0, y0, img0 in zip(x, y, images):\n",
        "              im = OffsetImage(img0, zoom=zoom)\n",
        "              ab = AnnotationBbox(im, (x0, y0), xycoords='data', frameon=True)\n",
        "              artists.append(ax.add_artist(ab))\n",
        "          ax.update_datalim(np.column_stack([x, y]))\n",
        "          ax.autoscale()\n",
        "          return artists\n",
        "\n",
        "      def plot_embedding(X, imgs, title=None):\n",
        "          x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
        "          X = (X - x_min) / (x_max - x_min)\n",
        "\n",
        "          plt.figure()\n",
        "          ax = plt.subplot(111)\n",
        "          for i in range(X.shape[0]):\n",
        "              plt.text(X[i, 0], X[i, 1], \".\", fontdict={'weight': 'bold', 'size': 9})\n",
        "          if hasattr(offsetbox, 'AnnotationBbox'):\n",
        "              imscatter(X[:,0], X[:,1], imgs, zoom=0.3, ax=ax)\n",
        "\n",
        "          plt.xticks([]), plt.yticks([])\n",
        "          if title is not None:\n",
        "              plt.title(title, fontsize=18)\n",
        "\n",
        "      tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
        "      X_tsne = tsne.fit_transform(X)\n",
        "      plot_embedding(X_tsne, imgs, \"t-SNE embeddings\")\n",
        "      if outFile is None:\n",
        "          plt.show()\n",
        "      else:\n",
        "          plt.savefig(outFile, bbox_inches='tight')\n",
        "      plt.close()\n",
        "\n",
        "  # Plot image reconstructions\n",
        "  def plot_reconstructions(imgs, imgs_reconstruct, outFile,\n",
        "                          range_imgs=[0, 255],\n",
        "                          range_imgs_reconstruct=[0, 1]):\n",
        "      # Create plot to save\n",
        "      assert len(imgs) == len(imgs_reconstruct)\n",
        "      fig = plt.figure(figsize=(20, 4))\n",
        "      fig.suptitle(\"Image Reconstructions\", fontsize=35)\n",
        "      n = min(len(imgs), 10)\n",
        "      for i in range(n):\n",
        "\n",
        "          # Plot original image\n",
        "          ax = plt.subplot(2, n, i + 1)\n",
        "          plt.imshow(imgs[i],\n",
        "                    vmin=range_imgs[0],\n",
        "                    vmax=range_imgs[1])\n",
        "          ax.get_xaxis().set_visible(False)\n",
        "          ax.get_yaxis().set_visible(False)\n",
        "\n",
        "          # Plot reconstructed image\n",
        "          ax = plt.subplot(2, n, n + i + 1)\n",
        "          plt.imshow(imgs_reconstruct[i],\n",
        "                    vmin=range_imgs_reconstruct[0],\n",
        "                    vmax=range_imgs_reconstruct[1])\n",
        "          ax.get_xaxis().set_visible(False)\n",
        "          ax.get_yaxis().set_visible(False)\n",
        "\n",
        "      if outFile is None:\n",
        "          plt.show()\n",
        "      else:\n",
        "          plt.savefig(outFile, bbox_inches='tight')\n",
        "      plt.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQtmBou5EEfa"
      },
      "source": [
        "import os\n",
        "import skimage.io\n",
        "from multiprocessing import Pool\n",
        "\n",
        "class CV_IO_utils:\n",
        "  # Read image\n",
        "  def read_img(filePath):\n",
        "      return skimage.io.imread(filePath, as_gray=False)\n",
        "\n",
        "  # Read images with common extensions from a directory\n",
        "  def read_imgs_dir(dirPath, extensions, parallel=True):\n",
        "      args = [os.path.join(dirPath, filename)\n",
        "              for filename in os.listdir(dirPath)\n",
        "              if any(filename.lower().endswith(ext) for ext in extensions)]\n",
        "      if parallel:\n",
        "          pool = Pool()\n",
        "          imgs = pool.map(CV_IO_utils.read_img, args)\n",
        "          pool.close()\n",
        "          pool.join()\n",
        "      else:\n",
        "          imgs = [read_img(arg) for arg in args]\n",
        "      return imgs\n",
        "\n",
        "  # Save image to file\n",
        "  def save_img(filePath, img):\n",
        "      skimage.io.imsave(filePath, img)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ispOKF2cEJwc"
      },
      "source": [
        "from multiprocessing import Pool\n",
        "from skimage.transform import resize\n",
        "\n",
        "# Apply transformations to multiple images\n",
        "class CV_transform_utils:\n",
        "    \n",
        "  def apply_transformer(imgs, transformer, parallel=True):\n",
        "      if parallel:\n",
        "          pool = Pool()\n",
        "          imgs_transform = pool.map(transformer, [img for img in imgs])\n",
        "          pool.close()\n",
        "          pool.join()\n",
        "      else:\n",
        "          imgs_transform = [transformer(img) for img in imgs]\n",
        "      return imgs_transform\n",
        "\n",
        "  # Normalize image data [0, 255] -> [0.0, 1.0]\n",
        "  def normalize_img(img):\n",
        "      return img / 255.\n",
        "\n",
        "  # Resize image\n",
        "  def resize_img(img, shape_resized):\n",
        "      img_resized = resize(img, shape_resized,\n",
        "                          anti_aliasing=True,\n",
        "                          preserve_range=True)\n",
        "      assert img_resized.shape == shape_resized\n",
        "      return img_resized\n",
        "\n",
        "  # Flatten image\n",
        "  def flatten_img(img):\n",
        "      return img.flatten(\"C\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ISzX4wfEM4v"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class utils:\n",
        "    # Get split indices\n",
        "  def split(fracs, N, seed):\n",
        "      fracs = [round(frac, 2) for frac in fracs]\n",
        "      if sum(fracs) != 1.00:\n",
        "          raise Exception(\"fracs do not sum to one!\")\n",
        "\n",
        "      # Shuffle ordered indices\n",
        "      indices = list(range(N))\n",
        "      random.Random(seed).shuffle(indices)\n",
        "      indices = np.array(indices, dtype=int)\n",
        "\n",
        "      # Get numbers per group\n",
        "      n_fracs = []\n",
        "      for i in range(len(fracs) - 1):\n",
        "          n_fracs.append(int(max(fracs[i] * N, 0)))\n",
        "      n_fracs.append(int(max(N - sum(n_fracs), 0)))\n",
        "\n",
        "      if sum(n_fracs) != N:\n",
        "          raise Exception(\"n_fracs do not sum to N!\")\n",
        "\n",
        "      # Sample indices\n",
        "      n_selected = 0\n",
        "      indices_fracs = []\n",
        "      for n_frac in n_fracs:\n",
        "          indices_frac = indices[n_selected:n_selected + n_frac]\n",
        "          indices_fracs.append(indices_frac)\n",
        "          n_selected += n_frac\n",
        "\n",
        "      # Check no intersections\n",
        "      for a, indices_frac_A in enumerate(indices_fracs):\n",
        "          for b, indices_frac_B in enumerate(indices_fracs):\n",
        "              if a == b:\n",
        "                  continue\n",
        "              if utils.is_intersect(indices_frac_A, indices_frac_B):\n",
        "                  raise Exception(\"there are intersections!\")\n",
        "\n",
        "      return indices_fracs\n",
        "\n",
        "  # Is there intersection?\n",
        "  def is_intersect(arr1, arr2):\n",
        "      n_intersect = len(np.intersect1d(arr1, arr2))\n",
        "      if n_intersect == 0: return False\n",
        "      else: return True"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvJ3I8wcERpf"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class AutoEncoder():\n",
        "\n",
        "    def __init__(self, modelName, info):\n",
        "        self.modelName = modelName\n",
        "        self.info = info\n",
        "        self.autoencoder = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "\n",
        "    # Train\n",
        "    def fit(self, X, n_epochs=50, batch_size=256):\n",
        "        indices_fracs = utils.split(fracs=[0.9, 0.1], N=len(X), seed=0)\n",
        "        X_train, X_valid = X[indices_fracs[0]], X[indices_fracs[1]]\n",
        "        self.autoencoder.fit(X_train, X_train,\n",
        "                             epochs = n_epochs,\n",
        "                             batch_size = batch_size,\n",
        "                             shuffle = True,\n",
        "                             validation_data = (X_valid, X_valid))\n",
        "\n",
        "    # Inference\n",
        "    def predict(self, X):\n",
        "        return self.encoder.predict(X)\n",
        "\n",
        "    # Set neural network architecture\n",
        "    def set_arch(self):\n",
        "\n",
        "        shape_img = self.info[\"shape_img\"]\n",
        "        shape_img_flattened = (np.prod(list(shape_img)),)\n",
        "\n",
        "        # Set encoder and decoder graphs\n",
        "        if self.modelName == \"simpleAE\":\n",
        "            encode_dim = 128\n",
        "\n",
        "            input = tf.keras.Input(shape=shape_img_flattened)\n",
        "            encoded = tf.keras.layers.Dense(encode_dim, activation='relu')(input)\n",
        "\n",
        "            decoded = tf.keras.layers.Dense(shape_img_flattened[0], activation='sigmoid')(encoded)\n",
        "\n",
        "        elif self.modelName == \"convAE\":\n",
        "            n_hidden_1, n_hidden_2, n_hidden_3 = 16, 8, 8\n",
        "            convkernel = (3, 3)  # convolution kernel\n",
        "            poolkernel = (2, 2)  # pooling kernel\n",
        "\n",
        "            input = tf.keras.layers.Input(shape=shape_img)\n",
        "            x = tf.keras.layers.Conv2D(n_hidden_1, convkernel, activation='relu', padding='same')(input)\n",
        "            x = tf.keras.layers.MaxPooling2D(poolkernel, padding='same')(x)\n",
        "            x = tf.keras.layers.Conv2D(n_hidden_2, convkernel, activation='relu', padding='same')(x)\n",
        "            x = tf.keras.layers.MaxPooling2D(poolkernel, padding='same')(x)\n",
        "            x = tf.keras.layers.Conv2D(n_hidden_3, convkernel, activation='relu', padding='same')(x)\n",
        "            encoded = tf.keras.layers.MaxPooling2D(poolkernel, padding='same')(x)\n",
        "\n",
        "            x = tf.keras.layers.Conv2D(n_hidden_3, convkernel, activation='relu', padding='same')(encoded)\n",
        "            x = tf.keras.layers.UpSampling2D(poolkernel)(x)\n",
        "            x = tf.keras.layers.Conv2D(n_hidden_2, convkernel, activation='relu', padding='same')(x)\n",
        "            x = tf.keras.layers.UpSampling2D(poolkernel)(x)\n",
        "            x = tf.keras.layers.Conv2D(n_hidden_1, convkernel, activation='relu', padding='same')(x)\n",
        "            x = tf.keras.layers.UpSampling2D(poolkernel)(x)\n",
        "            decoded = tf.keras.layers.Conv2D(shape_img[2], convkernel, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"Invalid model name given!\")\n",
        "\n",
        "        # Create autoencoder model\n",
        "        autoencoder = tf.keras.Model(input, decoded)\n",
        "        input_autoencoder_shape = autoencoder.layers[0].input_shape[1:]\n",
        "        output_autoencoder_shape = autoencoder.layers[-1].output_shape[1:]\n",
        "\n",
        "        # Create encoder model\n",
        "        encoder = tf.keras.Model(input, encoded)  # set encoder\n",
        "        input_encoder_shape = encoder.layers[0].input_shape[1:]\n",
        "        output_encoder_shape = encoder.layers[-1].output_shape[1:]\n",
        "\n",
        "        # Create decoder model\n",
        "        decoded_input = tf.keras.Input(shape=output_encoder_shape)\n",
        "        if self.modelName == 'simpleAE':\n",
        "            decoded_output = autoencoder.layers[-1](decoded_input)  # single layer\n",
        "        elif self.modelName == 'convAE':\n",
        "            decoded_output = autoencoder.layers[-7](decoded_input)  # Conv2D\n",
        "            decoded_output = autoencoder.layers[-6](decoded_output)  # UpSampling2D\n",
        "            decoded_output = autoencoder.layers[-5](decoded_output)  # Conv2D\n",
        "            decoded_output = autoencoder.layers[-4](decoded_output)  # UpSampling2D\n",
        "            decoded_output = autoencoder.layers[-3](decoded_output)  # Conv2D\n",
        "            decoded_output = autoencoder.layers[-2](decoded_output)  # UpSampling2D\n",
        "            decoded_output = autoencoder.layers[-1](decoded_output)  # Conv2D\n",
        "        else:\n",
        "            raise Exception(\"Invalid model name given!\")\n",
        "        decoder = tf.keras.Model(decoded_input, decoded_output)\n",
        "        decoder_input_shape = decoder.layers[0].input_shape[1:]\n",
        "        decoder_output_shape = decoder.layers[-1].output_shape[1:]\n",
        "\n",
        "        # Generate summaries\n",
        "        print(\"\\nautoencoder.summary():\")\n",
        "        print(autoencoder.summary())\n",
        "        print(\"\\nencoder.summary():\")\n",
        "        print(encoder.summary())\n",
        "        print(\"\\ndecoder.summary():\")\n",
        "        print(decoder.summary())\n",
        "\n",
        "        # Assign models\n",
        "        self.autoencoder = autoencoder\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    # Compile\n",
        "    def compile(self, loss=\"categorical_crossentropy\", optimizer=\"adam\"):\n",
        "        self.autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    # Load model architecture and weights\n",
        "    def load_models(self, loss=\"categorical_crossentropy\", optimizer=\"adam\"):\n",
        "        print(\"Loading models...\")\n",
        "        self.autoencoder = tf.keras.models.load_model(self.info[\"autoencoderFile\"])\n",
        "        self.encoder = tf.keras.models.load_model(self.info[\"encoderFile\"])\n",
        "        self.decoder = tf.keras.models.load_model(self.info[\"decoderFile\"])\n",
        "        self.autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "        self.encoder.compile(optimizer=optimizer, loss=loss)\n",
        "        self.decoder.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    # Save model architecture and weights to file\n",
        "    def save_models(self):\n",
        "        print(\"Saving models...\")\n",
        "        self.autoencoder.save(self.info[\"autoencoderFile\"])\n",
        "        self.encoder.save(self.info[\"encoderFile\"])\n",
        "        self.decoder.save(self.info[\"decoderFile\"])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dd5ij7vmw78"
      },
      "source": [
        "# Image Retrieval with 3 models (Simple Auto Encoder, Convolutional Auto Encoder, VGG19)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m6M4QpVFEdg",
        "outputId": "3b9d7aa1-2f25-417a-fe3d-5080a9c6b5b2"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Run mode: (autoencoder -> simpleAE, convAE) or (transfer learning -> vgg19)\n",
        "modelName = \"vgg19\"  # try: \"simpleAE\", \"convAE\", \"vgg19\"\n",
        "trainModel = True\n",
        "parallel = True  # use multicore processing\n",
        "\n",
        "# Make paths\n",
        "dataTrainDir = os.path.join(os.getcwd(), \"data\", \"train\")\n",
        "dataTestDir = os.path.join(os.getcwd(), \"data\", \"test\")\n",
        "outDir = os.path.join(os.getcwd(), \"output\", modelName)\n",
        "if not os.path.exists(outDir):\n",
        "    os.makedirs(outDir)\n",
        "\n",
        "# Read images\n",
        "extensions = [\".jpg\", \".jpeg\"]\n",
        "print(\"Reading train images from '{}'...\".format(dataTrainDir))\n",
        "imgs_train = CV_IO_utils.read_imgs_dir(dataTrainDir, extensions, parallel=parallel)\n",
        "print(\"Reading test images from '{}'...\".format(dataTestDir))\n",
        "imgs_test = CV_IO_utils.read_imgs_dir(dataTestDir, extensions, parallel=parallel)\n",
        "shape_img = imgs_train[0].shape\n",
        "print(\"Image shape = {}\".format(shape_img))\n",
        "\n",
        "# Build models\n",
        "if modelName in [\"simpleAE\", \"convAE\"]:\n",
        "\n",
        "    # Set up autoencoder\n",
        "    info = {\n",
        "        \"shape_img\": shape_img,\n",
        "        \"autoencoderFile\": os.path.join(outDir, \"{}_autoecoder.h5\".format(modelName)),\n",
        "        \"encoderFile\": os.path.join(outDir, \"{}_encoder.h5\".format(modelName)),\n",
        "        \"decoderFile\": os.path.join(outDir, \"{}_decoder.h5\".format(modelName)),\n",
        "    }\n",
        "    model = AutoEncoder(modelName, info)\n",
        "    model.set_arch()\n",
        "\n",
        "    if modelName == \"simpleAE\":\n",
        "        shape_img_resize = shape_img\n",
        "        input_shape_model = (model.encoder.input.shape[1],)\n",
        "        output_shape_model = (model.encoder.output.shape[1],)\n",
        "        n_epochs = 300\n",
        "    elif modelName == \"convAE\":\n",
        "        shape_img_resize = shape_img\n",
        "        input_shape_model = tuple([int(x) for x in model.encoder.input.shape[1:]])\n",
        "        output_shape_model = tuple([int(x) for x in model.encoder.output.shape[1:]])\n",
        "        n_epochs = 500\n",
        "    else:\n",
        "        raise Exception(\"Invalid modelName!\")\n",
        "\n",
        "elif modelName in [\"vgg19\"]:\n",
        "\n",
        "    # Load pre-trained VGG19 model + higher level layers\n",
        "    print(\"Loading VGG19 pre-trained model...\")\n",
        "    model = tf.keras.applications.VGG19(weights='imagenet', include_top=False,\n",
        "                                        input_shape=shape_img)\n",
        "    model.summary()\n",
        "\n",
        "    shape_img_resize = tuple([int(x) for x in model.input.shape[1:]])\n",
        "    input_shape_model = tuple([int(x) for x in model.input.shape[1:]])\n",
        "    output_shape_model = tuple([int(x) for x in model.output.shape[1:]])\n",
        "    n_epochs = None\n",
        "\n",
        "else:\n",
        "    raise Exception(\"Invalid modelName!\")\n",
        "\n",
        "# Print some model info\n",
        "print(\"input_shape_model = {}\".format(input_shape_model))\n",
        "print(\"output_shape_model = {}\".format(output_shape_model))\n",
        "\n",
        "# Apply transformations to all images\n",
        "class ImageTransformer(object):\n",
        "\n",
        "    def __init__(self, shape_resize):\n",
        "        self.shape_resize = shape_resize\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img_transformed = CV_transform_utils.resize_img(img, self.shape_resize)\n",
        "        img_transformed = CV_transform_utils.normalize_img(img_transformed)\n",
        "        return img_transformed\n",
        "\n",
        "transformer = ImageTransformer(shape_img_resize)\n",
        "print(\"Applying image transformer to training images...\")\n",
        "imgs_train_transformed = CV_transform_utils.apply_transformer(imgs_train, transformer, parallel=parallel)\n",
        "print(\"Applying image transformer to test images...\")\n",
        "imgs_test_transformed = CV_transform_utils.apply_transformer(imgs_test, transformer, parallel=parallel)\n",
        "\n",
        "# Convert images to numpy array\n",
        "X_train = np.array(imgs_train_transformed).reshape((-1,) + input_shape_model)\n",
        "X_test = np.array(imgs_test_transformed).reshape((-1,) + input_shape_model)\n",
        "print(\" -> X_train.shape = {}\".format(X_train.shape))\n",
        "print(\" -> X_test.shape = {}\".format(X_test.shape))\n",
        "\n",
        "# Train (if necessary)\n",
        "if modelName in [\"simpleAE\", \"convAE\"]:\n",
        "    if trainModel:\n",
        "        model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
        "        model.fit(X_train, n_epochs=n_epochs, batch_size=256)\n",
        "        model.save_models()\n",
        "    else:\n",
        "        model.load_models(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "# Create embeddings using model\n",
        "print(\"Inferencing embeddings using pre-trained model...\")\n",
        "E_train = model.predict(X_train)\n",
        "E_train_flatten = E_train.reshape((-1, np.prod(output_shape_model)))\n",
        "E_test = model.predict(X_test)\n",
        "E_test_flatten = E_test.reshape((-1, np.prod(output_shape_model)))\n",
        "print(\" -> E_train.shape = {}\".format(E_train.shape))\n",
        "print(\" -> E_test.shape = {}\".format(E_test.shape))\n",
        "print(\" -> E_train_flatten.shape = {}\".format(E_train_flatten.shape))\n",
        "print(\" -> E_test_flatten.shape = {}\".format(E_test_flatten.shape))\n",
        "\n",
        "# Make reconstruction visualizations\n",
        "if modelName in [\"simpleAE\", \"convAE\"]:\n",
        "    print(\"Visualizing database image reconstructions...\")\n",
        "    imgs_train_reconstruct = model.decoder.predict(E_train)\n",
        "    if modelName == \"simpleAE\":\n",
        "        imgs_train_reconstruct = imgs_train_reconstruct.reshape((-1,) + shape_img_resize)\n",
        "    CV_plot_utils.plot_reconstructions(imgs_train, imgs_train_reconstruct,\n",
        "                         os.path.join(outDir, \"{}_reconstruct.png\".format(modelName)),\n",
        "                         range_imgs=[0, 255],\n",
        "                         range_imgs_reconstruct=[0, 1])\n",
        "\n",
        "# Fit kNN model on training images\n",
        "print(\"Fitting k-nearest-neighbour model on training images...\")\n",
        "knn = NearestNeighbors(n_neighbors=5, metric=\"cosine\")\n",
        "knn.fit(E_train_flatten)\n",
        "\n",
        "# Perform image retrieval on test images\n",
        "print(\"Performing image retrieval on test images...\")\n",
        "for i, emb_flatten in enumerate(E_test_flatten):\n",
        "    _, indices = knn.kneighbors([emb_flatten]) # find k nearest train neighbours\n",
        "    img_query = imgs_test[i] # query image\n",
        "    imgs_retrieval = [imgs_train[idx] for idx in indices.flatten()] # retrieval images\n",
        "    outFile = os.path.join(outDir, \"{}_retrieval_{}.png\".format(modelName, i))\n",
        "    CV_plot_utils.plot_query_retrieval(img_query, imgs_retrieval, outFile)\n",
        "\n",
        "# Plot t-SNE visualization\n",
        "print(\"Visualizing t-SNE on training images...\")\n",
        "outFile = os.path.join(outDir, \"{}_tsne.png\".format(modelName))\n",
        "CV_plot_utils.plot_tsne(E_train_flatten, imgs_train, outFile)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading train images from '/content/data/train'...\n",
            "Reading test images from '/content/data/test'...\n",
            "Image shape = (128, 128, 3)\n",
            "Loading VGG19 pre-trained model...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "80150528/80134624 [==============================] - 1s 0us/step\n",
            "Model: \"vgg19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv4 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "=================================================================\n",
            "Total params: 20,024,384\n",
            "Trainable params: 20,024,384\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "input_shape_model = (128, 128, 3)\n",
            "output_shape_model = (4, 4, 512)\n",
            "Applying image transformer to training images...\n",
            "Applying image transformer to test images...\n",
            " -> X_train.shape = (250, 128, 128, 3)\n",
            " -> X_test.shape = (50, 128, 128, 3)\n",
            "Inferencing embeddings using pre-trained model...\n",
            " -> E_train.shape = (250, 4, 4, 512)\n",
            " -> E_test.shape = (50, 4, 4, 512)\n",
            " -> E_train_flatten.shape = (250, 8192)\n",
            " -> E_test_flatten.shape = (50, 8192)\n",
            "Fitting k-nearest-neighbour model on training images...\n",
            "Performing image retrieval on test images...\n",
            "Visualizing t-SNE on training images...\n"
          ]
        }
      ]
    }
  ]
}